# interactions

Partial slopes (change in y for each unit change in x, holding others constant) vs. marginal slopes (simple regression, only one x). Collinearity makes these values different...

For time series, the trigonometric dummies for seasonal strucure and those for calender effects tend to have no correlation with eachother. Also tend to have no correlation with the dummies of seasonality. So, time series presents an interesting case where partial slopes and marginal slopes are almost exactly the same.

If you have a model with seasonal structure with dummies, and a trend, you'll get certain partial slopes for the seasonal structure. If you add trigonometric pairs for calendar effects, then the dummy variables for calendar effects are barely correalted with seasonal structure, so the variables don't change that much when you introduce calendar effects.

Use a second regression to remove their influence on the previous X's. Strip out of X1 the influence of the other X's. So ,the partial slope of X1 is the same of the slope in simple regression. So, a partial slope is the effect AFTER you have taken into account the other x's. 

Each t test tests the influence of a single predictor beyond the other predictors. What does that x contribute to the model beyond the contributions of all the other X's taken sequentially. A "last in" test. Same interpretation holds for a partial F test. This assess variables being tested beyond that.

## Example

Simple regression. Sorted ~ Line. If one more line is added, the number of sorts/hour will increase by 1,4446. Other variables not inlcluded are free to vary.

$$
Sorts/Hour = 2,960 + 1,446Lines
$$

Lines, sorters, and hours. Now, the interpretation of the partial slope (826)... is that if you add one more line, and hold fixed the number of sorts and truckers, you increase sorts/hours by 826. Don't hire more workings, just add one more line. Borrow sorts and truckers from existing lines to form the new line. Partial and marginal slope are greatly different.

$$
Sorts/Hour = 739 + 826Lines + 95Sorters + 118Truckers
$$

In time series, the partial slopes we saw prior to the variables added are different. The partial slopes will change as more variables are added. 

### Interaction

Example: salaries for account level 1. Let y be salary, $x_1$ timeinposition, and $x-2$ a dummy for gender (0 for males and 1 for females). Regress y on $x_1$ for males only and a seperate regression for females. Examine two fitted lines, comparing their intercepts and their slopes. A difference in intercepts signals a difference in entry level salaries, and difference in slopes indicate difference rates of pay increase for two sexes. A difference in slopes signals interaction. That is, the relationship between y (salary) and one of the independent variables, $x_1$, time in position, differs according to the value of the other independent variable, $x_2$. 

In this example, there is interaction if the relationship between salary and time in position differs according to the other x variable (gender).

Instead of fitting two lines seperately, pool the male and female data to estimate a common erro variables with more degrees of freedom than seperate regressions permit. Leads to more powerful tests, narrower confidence intervals and prediction intervals. 

Why is the product the proper method to incorporate interaction? See:

$$
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_sx_ax_2 + \epsilon \\
= \beta_0 + \beta_1timeinposition + \beta_2gender + \beta_3timeinposition*gender + \epsilon
$$

For males this model is 

$$
y = \beta_0+\beta_1timeinposition + \epsilon
$$


For females this model is 

$$
y = \beta_0+\beta_2 + (\beta_1+\beta_3)timeinposition + \epsilon
$$
For males, the intercept is $\beta_0$ and the slope attached to timeinposition is $\beta_1$. For females, these are the $\beta_0 + \beta_2$ and $\beta_1 +\beta_3$ respectively. If the paraemter $\beta_3$ is present, then we say that there is interaction between timeinposition and gender. This means that the impact of timeinposition upon salary depends on gender. 

This allows us to test whether the same or entry level salaries are same, and whether rates of pay increase are the same. If we test if entry level salaries are equal, then hyp = B2 = 0. If not, then females sig differently. For rate of pay increase, B1 is for males, but females if B1 + B3, so test whether B3 is equal to 0 or not in the alternative. 

Interaction means that the relationship between ssalary and time in position, if interaction is present, then that relationship is sig. different for the two genders. That is the case when $B_3$ is sig different from 0. 


### example

The baseline trend is when neither one of them is on duty. The interactions show when they are or are not on duty (add example)

### Durbin-Watson statistic

Used to test the null of uncorrelated errors in a time series regression model, against the alternative that errors follow a first-order autoregression with positive lag one correlation


$$
y_ = \beta_0 + \beta_1x_{1t} + \beta_2x_{2t} + ...+ \beta_kx_{kt} + \epsilon_t
$$

where the errors $\epsilon_t$ follows a first-order autoregressive model

$$
\epsilon_t = \rho\epsilon_{t-1} + \alpha_t, -1 < \rho <1
$$
$$
\epsilon_t = \rho\epsilon_{t-1} + \alpha_t, -1 < \rho <1
$$

Durbin-Watson tests the null $H_0: \rho = 0$ against the alternatie that $\rho$ is positive.

Calculated with Durbin-Watson statistic:

$$
DW = \frac{\sum_{t=2}^{n}(e_t - e_{t-1})^2}{\sum_{t=1}^{n}e_t^2}
$$

and this is approximately equal to 2(1-r) where

$$
r = \frac{\sum_{t=2}^{n}e_te_{t-1}}{\sum_{t=1}^{n}e_t^2}
$$

is the sample correlation between adjacent residuals and is an estimate of $\rho$. The null is rejected if 

$$
DW > d_t
$$


## Weekly Garbage Deposites


### Simple model 

with only a trend structure, dummy variables for two people, and dummy variables to estimate seasonal structure 

Fundamental pair corresponds to a period of 365/7, which is 57 and 1/7. Takes 57 and 1/7 to span a week. Use 365.25 for leap year. The fundamental component has a period... the amount of time it takes to complete one cycle is. 52 1/7 weeks. First overtone inorporate j=t, then j=3, j=4... 
$$
y_t = \beta_0 + \beta_t + \beta_2t^2 + \beta_3t^3 + \beta_4M_t + \beta_5G_t + \beta_6MG_t + \sum_{j=1}^{4}(\gamma_jcos\frac{2\pi7jt}{365} + \delta_jsin\frac{2\pi7jt}{365}) + \epsilon_t
$$

Dummy variables use the 0,1 convension, where M is 1 is Marlene on during week t, and 0 otherwise... MG for both Marleen and George on duty during week 1. 
## Code

Purpose is to estimate how much stolen. The value attached to marlene suggests that the amount of shortage on the tonnage was 164.9, and for George, was 152.5. When two working together, we add these three, and the coefficient is positive, same order of magnitude. Some of the cosine/sin terms are sig...

From this week, we can get week by week losses in tonnage. Interpretation: in a week when Marlone alone working, the amount lost is 1.64... Add them up to get estimate of total amount of loss tonnage, and revenue...

Modestly significant lag 15 residual autocorrelaiton. Otherwise, the plot reveals no remaining autocorrelation. The next plots show the estimated seasonal and trend structures.

```{r}
garbage <- read.csv("/cloud/project/data/Garbage (1).txt")
attach(garbage)
head(garbage)
freq <- 14*pi/365
time <- as.numeric(1:length(tonnage))
time2 <- time*time; time3 <- time*time2
cosm <- matrix(nrow = length(tonnage), ncol = 8)
sinm <- matrix(nrow = length(tonnage), ncol = 8)
for (j in 1:8){
  cosm[,j]<- cos(freq*j*time)
  sinm[,j] <- sin(freq*j*time)
}
model1 <- lm(tonnage~time+time2+time3+marlene+george+mg+cosm[,1]+sinm[,1]+cosm[,2
]+sinm[,2]+cosm[,3]+sinm[,3]+cosm[,4]+sinm[,4]);summary(model1)
```

No strong outliers. Not that much trending.. and don't go to higher order polynomial. Minor heteroskedasticity. Volatility changes a bit.. Week 50 to week 80, it is common in terms of up and down motion. Modest amount of change in volatility. 

ACF of residuals: have we reduced to white noise? Significant lag 15 correlation, but not very far... Model essentially reduces to white noise. No remaining autocorrelation structure. 
```{r}
resids <- resid(model1)
qqnorm(resids)
qqline(resids)
plot(ts(resids))
acf(ts(resids))

```

Estimate of the seasonal pattern. Sicne the data start at end of 84, there is a dip in collection at early Jan, then reach a peak in the spring, smaller peak in the summer. Then, big peak in the fall, and the peak in the fall is more substantial than in the spring. This is the static seasonal estimation, and static because we have the same pattern from one year to the next. We need 157 to have three complete years.
```{r}
xmatrix<-
matrix(c(cosm[,1],sinm[,1],cosm[,2],sinm[,2],cosm[,3],sinm[,3],cosm[,4]
,sinm[,4]),ncol=8) 
betaest<-coef(model1)[8:15] 

plot(ts(xmatrix%*%betaest),xlab="Time",ylab="Seasonal",main="Estimated 
Seasonal from Model 1") 
 

```

In next fit, model 2, we add four more overtone pairs to the seasonal component. Are these significant beyond the original four? Keep time cubed, and for the additional trig pairs, none are sig. R squared raised by about 3.25% from previous model. 

```{r}
model2<- lm(tonnage~time+time2+time3+marlene+george+mg+cosm[,1]+sinm[,1]+cosm[,
2]+sinm[,2]+cosm[,3]+sinm[,3]+cosm[,4]+sinm[,4]+cosm[,5]+sinm[,5]+cosm
[,6]+sinm[,6]+cosm[,7]+sinm[,7]+cosm[,8]+sinm[,8]);summary(model2) 
```

Partial F test to compare model 1 and model 2. This tests the eight additional variables in the four additional cosine/sin pairs. 

```{r}
anova(model1, model2)
summary(model1)$r.squared
summary(model2)$r.squared
```

Partial F stat calcualted with

$$
\frac{0.63902 -0.63902/8}{(1-0.63902)/(155-22-1)}
$$
The addition of the 4 trigonometric pairs gives this nuance. 

```{r}
xmatrix3<-
matrix(c(cosm[,1],sinm[,1],cosm[,2],sinm[,2],cosm[,3],sinm[,3],cosm[,4]
,sinm[,4],cosm[,5],sinm[,5],cosm[,6],sinm[,6],cosm[,7],sinm[,7],cosm[,8
],sinm[,8]),ncol=16) 
betaest<-coef(model2)[8:23] 
plot(ts(xmatrix3%*%betaest),xlab="Time",ylab="Seasonal",main="Estimated 
Seasonal from Model 2") 
```


### model 3

Back to 4 cosine sin pairs, but this is include interaction of M, G and the trend and MG.

In the weeks when marele on duty, time:marleene is adjustment to time. The line time2:marlene is an adjustment to the value of time2. Likewise for George and MG. We get four different trends, a trend when nither of them is on duty, one when Marlene alone on duty, and one when George alone on duty.

```{r}
model3<- lm(tonnage~time+time2+time3+marlene+george+mg+cosm[,1]+sinm[,1]
            +cosm[,2]+sinm[,2]+cosm[,3]+sinm[,3]+cosm[,4]+sinm[,4]+
              marlene*time+marlene*time2+george*time+george*time2+mg*time+mg*time2)

summary(model3) 

```



### Calculating the six interaction terms.


### Visualizing Three Trends

Blue trend is G, Red is Marlene. At the far left, Marlene usually stealing more than George. As time goes on, G stealing more.
```{r}
nmatrix<-matrix(c(rep(1,length(tonnage)),time,time2,time3),ncol=4) 
nbeta<-c(325.082,2.5825,-0.0371196,0.00016114) 
mbeta<-c(227.422,0.14524,-0.021722,0.00016114) 
gbeta<-c(273.269,-0.03985,-0.024129,0.00016114) 

plot(ts(nmatrix%*%nbeta),type="l",lty=1,lwd=2,col="black",ylim=c(0,500
),xlab="Time",ylab="Trend",main="Estimated Trends from Model 3") 
lines(ts(nmatrix%*%mbeta),type="l",lty=2,lwd=2,col="red") 
lines(ts(nmatrix%*%gbeta),type="l",lty=3,lwd=2,col="blue") 

plot(ts(resid(model3)))
acf(ts(resid(model3)))

```

Model extension of the second and third models, including eight trigonometric pairs and the interactions

```{r}
model4<- lm(tonnage~time+time2+time3+marlene+george+mg+cosm[,1]+sinm[,1]+cosm[,
2]+sinm[,2]+cosm[,3]+sinm[,3]+cosm[,4]+sinm[,4]+cosm[,5]+sinm[,5]+cosm
[,6]+sinm[,6]+cosm[,7]+sinm[,7]+cosm[,8]+sinm[,8]+marlene*time+marlene
*time2+george*time+george*time2+mg*time+mg*time2);summary(model4) 

```

F test comparing model 4 and model 1

```{r}
anova(model1, model4)
plot(ts(resid(model4)))

```

Estimation of the lost tonnage and lost revenue, with a 95% confidence interval

```{r}
xmatrixtons <- matrix(c(marlene, george,mg), ncol = 3)
betaesttons <- coef(model1)[5:7]
-sum(xmatrixtons%*%betaesttons)

xmatrixrev <- matrix(c(marlene*fee, george*fee, mg*fee), ncol = 3)
-sum(xmatrixrev%*%betaesttons)
r<-cbind(sum(marlene*fee),sum(george*fee),sum(mg*fee)) 
sdest<-sqrt(r%*%vcov(model1)[5:7,5:7]%*%t(r)) 
sdest 

## confidence interval calculations
lcl <--sum(xmatrixrev%*%betaesttons)-1.96*sdest
ucl<--sum(xmatrixrev%*%betaesttons)+1.96*sdest 
```








